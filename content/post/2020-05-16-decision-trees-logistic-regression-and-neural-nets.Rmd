---
title: Decision Trees, Regression, and Neural Nets
author: Scott Miner
date: '2020-05-16'
slug: decision-trees-logistic-regression-and-neural-nets
categories:
 - Predictive Analytics
 - Logistic Regression
 - Decision Trees
 - Neural Networks
tags:
 - Predictive Analytics
 - Logistic Regression
 - Decision Trees
 - Neural Networks
bibliography: neuralNetsBib.bib
link-citations: true
---

<style>
p.caption {
  font-size: 0.6em;
  font-style: italic;
}
</style>

This week I am comparing three different predictive models: decision trees, regression, and neural nets.  @abbott describes artificial neural nets (ANNs) as being like linear regression and logistic regression in that they both require numeric inputs and cannot process missing records.

Practitioners convert categorical data to dummy variables and use typical imputation methods to impute missing values.  The most common methodology to create dummy variables is to create one column for each value of the categorical variable less one.  For instance, if a categorical variables contains ten levels, create nine dummy variables [@abbott].

Decision trees, on the other hand, can process both nominal and numeric inputs.  Many decision tree algorithms process missing data autonomously.  Decision trees are non-parametric, meaning they make no assumptions about the distributions of the input variables or target variables.  Moreover, decision trees can process continuous variables that do not adhere to specific distributions and continuous variables that contain outliers [@abbott].

ANNs are more precise predictors than both forms of regression and decision trees.  Decision trees are nonlinear predictors.  ANNs, however, are immensely powerful in their ability to create smooth nonlinear decision boundaries.  Figure 1 displays the results of applying ANNs to a data set with seven target variables. Notice the smooth nonlinear decision boundaries the ANNs create [@abbott].

![Figure 1. Neural network decision regions on nasadata (Abbott, 2014, p. 253).](/post/2020-05-16-decision-trees-logistic-regression-and-neural-nets_files/Annotation 2020-05-14 235311.jpg){width=80% height=20%}

ANNs, however, are prone to overfitting on occasion and sometimes do not converge to optimum solutions.  For these reasons, it is usually helpful to build models that incorporate multiple algorithms [@abbott].

@neuralNets built models from 373,246 records of 2014-2015 Swiss health insurance claims data to predict future cost increases.  The authors developed predictive models using logistic regression (LR), boosted decision trees (BDTs), and feedforward neural networks (FNNs) to predict “whether patient’s total costs would increase or decrease in 2015, based on their characteristics in 2014” (p. 2).

Additionally, the authors performed a feature importance analysis to determine which features were most important in predicting cost increases.  The authors found BDTs to be most accurate in predicting the outcome variable, with an overall accuracy of 67.6% and an area under the curve (AUC) score of 0.74.  Moreover, the authors reduced the number of variables from 747 to 36 with only a 0.5% loss in accuracy.

Figure 2 displays a plot of the backward deletion and the corresponding accuracy levels, where the vertical black line represents the 36 most important features.

![Figure 2. Backward deletion and corresponding accuracy levels.  Reprinted from "Prediction of health care expenditure increase: How does pharmacotherapy contribute?," by Jödicke et al., 2019, BMC Health Services Research, 19(1).](/post/2020-05-16-decision-trees-logistic-regression-and-neural-nets_files/Annotation 2020-05-14 235327.jpg){width=80% height=20%}

Figure 3 displays the AUC curve for each of the predictive models.

![Figure 3. Comparison of prediction performance.  Reprinted from "Prediction of health care expenditure increase: How does pharmacotherapy contribute?," by Jödicke et al., 2019, BMC Health Services Research, 19(1).](/post/2020-05-16-decision-trees-logistic-regression-and-neural-nets_files/Annotation 2020-05-14 235334.jpg){width=80% height=20%}

In conclusion, DTs, regression models, and neural nets each have their advantages and disadvantages.  ANNs are often more accurate than DTs and regression models.  However, this is not always the case, as the study by @neuralNets et al. demonstrates.  An additional way to increase the accuracy of decision trees, if model interpretation is not important, is to build ensembles of trees [@abbott].

# References
