---
title: Analyzing Customer Reviews Using Text Mining to Predict Their Behaviour
author: Scott Miner
date: '2020-03-29'
slug: analyzing-customer-reviews-using-text-mining-to-predict-their-behaviour
categories:
  - Text-Mining
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.asp = 0.618, out.width = "70%", fig.align = "center", warning = -1)
library(qdap)
library(dplyr)
library(tm)
library(wordcloud)
library(plotrix)
library(dendextend)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(reshape2)
library(quanteda)
```

I found a text mining tutorial online at https://medium.com/analytics-vidhya/customer-review-analytics-using-text-mining-cd1e17d6ee4e.

The article is by Sowmya Vivek.  The tutorial aims to analyze customer reviews to predict if customers will recommend products.  

We obtain the data set from Kaggle. (https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/home).

The data is from a Women's Clothing E-Commerce site.

The dataset contains 23,486 rows and 10 feature variables.

Each row corresponds to a customer review and includes the following variables:

* Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.
* Age: Positive Integer variable of the reviewers age.
* Title: String variable for the title of the review.
* Review Text: String variable for the review body.
* Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.
* Recommended IND: Binary variable stating where the customer recommends the product.  1 is recommended, 0 is not recommended.
* Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.
* Division Name: Categorical name of the product high level division.
* Department Name: Categorical name of the product department name.
* Class Name: Categorical name of the product class name.

We will use the following techniques to understand various aspects of text mining:

* *Exploratory analysis of text data* (Review Text) individually and based on how it impacts the customer decision to recommend the product (Recommended IND)

* *Classification models* built based on the review text as the independent variable to predict whether a customer recommends a product.

Additionally, the focus is to understand text mining and to understand differences between customers who recommend a product and those who don't rather than predicting the customer action based on the review.  We are focusing more on variable importance and coefficient scores of the models than model performance measures.

We have 2 broad categories of text mining approaches:

* *semantic parsing* where the word sequence, word usage, as noun or verb, hierarchical word structure etc matters.

* *Bag of words* where all the words are analysed as a single token and order does not matter.

* Step 1: Text extraction & creating a corpus
* Step 2: Text pre-processing
* Step 3: Creating the document term-matrix (DTM) & term-document-matrix (TDM)
* Step 4: Exploratory text analysis (word cloud, polarized plot and dendrograms
* Step 5: Feature extraction by removing sparsity
* Step 6: Classification models

# STEP1 - Text extraction & creating a corpus

#### Initial setup

The packages required for mining are loaded in the R environment:

```{r packages, echo=TRUE, eval=FALSE}
# install.packages("ggthemes")
# install.packages(qdap)
# install.packages(dplyr)
# install.packages(tm)
# install.packages(wordcloud)
# install.packages(plotrix)
# install.packages(dendextend)
# install.packages(ggplot2)
# install.packages(ggthemes)
# install.packages(RWeka)
# install.packages(reshape2)
# install.packages(quanteda)
library(qdap)
library(dplyr)
library(tm)
library(wordcloud)
library(plotrix)
library(dendextend)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(reshape2)
library(quanteda)
```

I had to go: here https://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/ to install the correct version of Java.

Additionally, I went to this website: https://www.java.com/en/download/manual.jsp

After that, I resolved the error "JAVA_HOME cannot be determined from the Registry" by installing a Java version.

After loading the required packages, we set the working directory and load in the csv files.

```{r read-csv-files}
review=read.csv("~/OneDrive/Documents/datasets/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv", stringsAsFactors = FALSE)
names(review)

```

**Review.Text** contains the customer reviews received for various products.

1.  Review.text is converted into a collection of text documents or a **"Corpus"**.
2.  To convert the text into a corpus, we use the "tm" package in R.
3.  We pass a source object as a paramter to the Corpus method.
4.  The source object is similar to an abstract input location.  The source we use here is a "Vectorsource" which inputs only character vectors.
5.  The Review.text column is now converted to a corpus that we call "corpus_review"

```{r make-vector-source-and-corpus}
corpus_review=Corpus(VectorSource(review$Review.Text))
```

## STEP2 - Text Pre-processing

Common pre-processing steps:

1. Convert to lower case--this way, if there are 2 words "Dress" and "dress", we will convert to a single entry "dress"

```{r convert-to-lower-case}
corpus_review=tm_map(corpus_review, tolower)
```

2. Remove Punctuation: `corpus_review=tm_map(corpus_review, removePunctuation)`

```{r remove-punctuation}
corpus_review=tm_map(corpus_review, removePunctuation)
```

3. Remove stopwords: If stopworse are not removed, they will appear in all the frequently used words list, and will not give the correct picture of the core words used in the text. `corpus_review=tm_map(corpus_review, removeWords, stopwords("english"))`
    + Also, remove custom stopwords, which are words specific to the dataset that may not add value to the text.

### Stemming a document
```{r remove-stop-words}
corpus_review=suppressWarnings(tm_map(corpus_review, removeWords, stopwords("english")))
# remove custom stop words
corpus_review=suppressWarnings(tm_map(corpus_review, removeWords, c("also","get","like","company","made","can","im","dress","just","i")))
```

Stemming is the process of reducing inflected (or derived) words to their word stem, base or root form-generally a written word form.

`SnowballC` is used for document stemming.  Complicated, complication, and complicate will reduce to complicat after stemming.  This is to ensure that the same word is not repeated as multiple version is the DTM and TDM and only have the root of the word represented in the DTM and TDM.

```{r stem-document}
corpus_review=tm_map(corpus_review, stemDocument)

# view the corpus content
corpus_review[[8]][1]
```
The corpus object in R is a nested list, we can use the r syntax for lists to view contents of the corpus.

#### Frequently used words

The text corpus is now cleaned and only contains the core words required for text mining.  The next step is exploratory analysis.  We first need to identify the most frequently used words.

```{r most-frequent-terms}
term_count <- freq_terms(corpus_review, 20)

# plot 20 most frequent terms
plot(term_count)
```

#### STEP3 - Create the CTM & TDM from the corpus

The pre-processed and cleaned up corpus is converted into a matrix called the document term matrix.

In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.

An easy way to start analyzing the information is to change the DTM/TDM into a simple matrix using `as.matrix()`.

```{r DTM-TDM}
review_dtm <- DocumentTermMatrix(corpus_review)
review_tdm <- TermDocumentMatrix(corpus_review)
```

#### Using the TDM to identify frequent terms

```{r use-the-TDM-to-identify-frequent-terms}
# Convert TDM to matrix
review_m <- as.matrix(review_tdm)
# Sum rows and frequency data frame
review_term_freq <- rowSums(review_m)
# Sort term_frequency in descending order
review_term_freq <- sort(review_term_freq, decreasing = T)
# View the top 10 most common words
review_term_freq[1:10]

```

#### Exploratory text analysis

```{r Plot-a-barchart-of-the-20-most-common-words}
# plot a barchart of the 20 most common words
barplot(review_term_freq[1:20], col = "steel blue", las = 2)
```

#### Word clouds

Word clouds vary the size of the words, based on the frequency.

```{r word-clouds}
review_word_freq <- data.frame(term = names(review_term_freq),
                               num = review_term_freq)

# Create a wordcloud for the values in word_freqs
suppressWarnings(wordcloud(review_word_freq$term, review_word_freq$num,
          max.words = 50, colors = "red"))
```

The word cloud can also receive a set of colors or a color palette as input to distinguish betweehn the more and the lesser frequent words in the cloud.

```{r word-cloud-with-specified-colors}
# Print the word cloud with the specified colors
suppressWarnings(wordcloud(review_word_freq$term, review_word_freq$num,
          max.words = 50, colors = c("aquamarine","darkgoldenrod","tomato")))
```


#### Comparison of corpus

Analyze the difference in keywords between those who recommend and those who don't recommend the product.

We create 2 corpora -- one for Recommend-yes and another for Recommend-no.

We repeat all the pre-processing steps done previously for both corpora.

Next, the frequenly used words are plotted as separate bar plots and word clouds for each of the corpora to understand the difference in the words used by customers who recommend a product vs those who don't.

#### Word clouds for comparison

Another way to compare word sets is to combine the corpora for yes and no and create comparison clouds which display both the sets of words in the same cloud.

We use 2 more versions of the word cloud -- the **commonality cloud** and the **comaprison cloud**.

The commonality combines words and plots a word cloud

```{r combine-both-corpora}

all_yes <- review %>%
    filter(Recommended.IND == 1) %>%
    select(Review.Text) %>%
    paste(collapse = "")

all_no <- review %>%
    filter(Recommended.IND == 0) %>%
    select(Review.Text) %>%
    paste(collapse = "")

all_combine <- c(all_yes, all_no)


corpus_review_all=Corpus(VectorSource(all_combine))

## Pre-processing - all
# Convert to lower-case
corpus_review_all=tm_map(corpus_review_all, tolower)

# Remove punctuation
corpus_review_all = suppressWarnings(tm_map(corpus_review_all, removePunctuation))

# Remove stopwords
corpus_review_all=suppressWarnings(tm_map(corpus_review_all, removeWords, stopwords("english")))
corpus_review_all=suppressWarnings(tm_map(corpus_review_all, removeWords, c("also","get",
                                                           "like","company","made","can","im",
                                                           "dress","just","i")))

# Stem document

corpus_review_all=suppressWarnings(tm_map(corpus_review_all, stemDocument))
review_tdm_all <- TermDocumentMatrix(corpus_review_all)
all_m=as.matrix(review_tdm_all)
colnames(all_m)=c("Yes","No")

# Sum rows and frequency data frame
review_term_freq_all <- rowSums(all_m)
review_word_freq_all <- data.frame(term=names(review_term_freq_all),
                                   num = review_term_freq_all)

# Make commonality cloud
suppressWarnings(commonality.cloud(all_m,
                  colors = "steelblue1",
                  max.words = 50))
```

```{r comparison-cloud}
# Create the comparison cloud
suppressWarnings(comparison.cloud(all_m,
                 colors = c("green", "red"),
                 max.words = 50))
```

#### Polarized tag plot

A polarized tag plot is an improved version of the commonality cloud.  It determines the frequency of a term used in both the corpora under comparison.

The matrix is created with all the common words using a subset to ensure that it contains only words occurring in both the classes.  The matrix has another column for the absolute difference between both the corpora for each word and the plot is made.

```{r polarized-tag-plot}
# Identify terms shared by both documents
common_words <- subset(all_m, all_m[,1] > 0 & all_m[, 2] > 0)

# calculate common words and difference
difference <- abs(common_words[, 1] - common_words[, 2])
common_words <- cbind(common_words, difference)
common_words <- common_words[order(common_words[, 3],
                                   decreasing = T), ]
head(common_words)
```

```{r continue}
top25_df <- data.frame(x = common_words[1:25, 1],
                       y = common_words[1:25, 2],
                       labels = rownames(common_words[1:25, ]))

# Make pyramid plot
pyramid.plot(top25_df$x, top25_df$y,
             labels = top25_df$labels,
             main = "Words in Common", 
             gap = 2000,
             laxlab = NULL,
             raxlab =  NULL,
             unit = NULL,
             top.labels = c("Yes",
                            "Words",
                            "No")
)
             
```

#### Simple word clustering

Word clustering is used to identify word groups used together.  Words clusters are visualized with dendrograms.

```{r plot-a-dendrogram}
review_tdm2 <- removeSparseTerms(review_tdm, sparse = 0.9)
hc <- hclust(d = dist(review_tdm2, method = "euclidean"), method = "complete")

# Plot a dendrogram
plot(hc)
```

Soft, material and comfort have been used together.  

### Word Associations

We will demonstrate correlation between various words and the word "fit".

```{r word-associations}
# Create associations
associations <- findAssocs(review_tdm, "fit", 0.05)

# Create associations_df
associations_df <- list_vect2df(associations)[, 2:3]

# Plot the associations_df values
ggplot(associations_df, aes(y = associations_df[,1])) +
  geom_point(aes(x = associations_df[, 2]),
             data = associations_df, size = 3) +
  ggtitle("Word Associations to 'fit'") +
  theme_gdocs()
```

The word "fit" has the greatest association with "perfect" and "size", which is the positive aspect of the product.  The third highest word associated with "fit" is "loos" which indicates the negative aspect of the product.

```{r bi-grams}

## Create bi-grams
review_bigram <- tokens(review$Review.Text) %>%
  tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
  tokens_remove(stopwords("english"), padding = TRUE) %>%
  tokens_ngrams(n = 2) %>%
  dfm()
topfeatures(review_bigram)
```

## Create tri-grams
```{r tri-grams}

## Create tri-gramms
review_trigram <- tokens(review$Review.Text) %>%
  tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
  tokens_ngrams(n = 3) %>%
  dfm()
topfeatures(review_trigram)
```

```{r barplot -of-frequently-used-bi-grams}

```

