---
title: Predict the product recommended
author: Scott Miner
date: '2020-03-29'
slug: lasso-logistic-regression
categories:
  - Text-Mining
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.asp = 0.618, out.width = "70%", fig.align = "center", warning = -1)
## Load the required libraries
library(quanteda)
library(irlba)
library(e1071)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(biclust)
library(igraph)
library(fpc)
library(tm)
library(randomForest)
library(glmnet)
```

#### Feature Extraction

We will try 3 of the most popular classification algorithms -- CART, Random forest and Lasso logistic regression.

```{r packages, eval=FALSE, echo=TRUE}

## Load the required libraries
library(irlba)
library(e1071)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(biclust)
library(igraph)
library(fpc)
library(Rcampdf)
```

#### Tokenisation

Tokenisation is the process of decomposing text into distinct pieces or tokens.

Once tokenisation is done, it is possible to construct a dataframe where each row represents a document and each column represents a token and each cell gives the count of the token for a document.

This is the DTM that we learnt in the previous section.

```{r pre-processing}
review=read.csv("~/OneDrive/Documents/datasets/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv", stringsAsFactors = FALSE)

# Tokenize description
reviewtokens=tokens(review$Review.Text,what="word",
                    remove_numbers=TRUE,
                    remove_punct=TRUE,
                    remove_symbols=TRUE,
                    split_hyphens=TRUE)

# Lowercase the tokens
reviewtokens=tokens_tolower(reviewtokens)

# remove stop words and unnecessary words
rmwords <- c("dress", "etc", "also", "xxs", "xs", "s")
reviewtokens=tokens_select(reviewtokens, stopwords(),selection = "remove")
reviewtokens=tokens_remove(reviewtokens,rmwords)

# Stemming tokens
reviewtokens=tokens_wordstem(reviewtokens,language = "english")
reviewtokens=tokens_ngrams(reviewtokens,n=1:2)
```

Tokens are now converted to a document frequency matrix and treated for sparsity.

```{r treat-for-sparsity}
reviewtokensdfm=dfm(reviewtokens,tolower = FALSE)

# Remove sparsity
reviewSparse <- convert(reviewtokensdfm, "tm")
tm::removeSparseTerms(reviewSparse, 0.7)

# Create the dfm
dfm_trim(reviewtokensdfm, min_docfreq = 0.3)
x=dfm_trim(reviewtokensdfm, sparsity = 0.98)
```

#### STEP6 Building the Classification Models 

We now have the pre-processed, treated and ready to be used for classification - dataframe.

The following steps are carried out:

1. Convert the dfm into a dataframe
2. Add the "Y" variable to the converted dataframe
3. Cleanup the column names so that they are compatible with r
4. Remove the original review.text column since we already have this as converted columns.

```{r building-classification-models}
## Setup a dataframe with features
df=convert(x,to="data.frame")

##Add the Y variable Recommend.IND
reviewtokensdf=cbind(review$Recommended.IND,df)
head(reviewtokensdf)

## Cleanup names
names(reviewtokensdf)[names(reviewtokensdf) == "review$Recommended.IND"] <- "recommend"
names(reviewtokensdf)=make.names(names(reviewtokensdf))
head(reviewtokensdf)

## Remove the original review.text column
reviewtokensdf=reviewtokensdf[,-c(2)]
head(reviewtokensdf)
reviewtokensdf$recommend=factor(reviewtokensdf$recommend)
```

#### Cart Model

First, we will use the CART algorithm for classification.  First the complete tree is built and the optimum cp value is identified for which the error is minimum.  Then, this cp value is used to obain the pruned tree.  The pruned tree is plotted to understand the classification.

```{r cart-model}
## Build the CART model

tree=rpart(formula = recommend ~ ., data = reviewtokensdf,
           method="class", control=rpart.control(minsplit = 200, minbucket = 30, cp = 0.0001))

printcp(tree)
plotcp(tree)

## Prune down the tree
bestcp=tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
bestcp

ptree=prune(tree,cp=bestcp)

rpart.plot(ptree,cex = 0.6)
prp(ptree, faclen = 0, cex = 0.5, extra = 2)
```

AS can be seen from the tree plot, words like "return", "disappoint", "back", "huge", etc are used by unhappy customers -- i.e., customers who do not recommend the product.  

#### Random forest

Next, we use Random forest.  We examine the varimp plot of the randomforest model to understand which words affect the classification the most.

```{r random-forest, eval=FALSE, echo=TRUE}

reviewRF=randomForest(recommend~., data=reviewtokensdf)
varImpPlot(reviewRF, cex=.7)
```

In sync with the CART model, the varimp plot of the Random forest model also indicates that "return", and "disappoint" are the most important variables.

#### Lasso logistic Regression

The main challenge associated with text mining data frames is the very high number of columns or features.

WE will use Regularisation using lasso for feature reduction.

The odds ratio of the logistic regression model will throw several insights on classification.

```{r lasso-logistic-regression}
# requires library(glmnet)

# convert training data to matrix format
x <- model.matrix(recommend~.,reviewtokensdf)

# convert class to numerical variable
y <- as.numeric(reviewtokensdf$recommend)

# perform grid search to find optimal value of lambda
cv.out <- cv.glmnet(x,y,alpha=1,family="binomial",type.measure = "mse")

# plot result
plot(cv.out)

```

```{r lambda, echo=TRUE, eval=FALSE}
# min value of lambda
lambda_min <- cv.out$lambda.min

# best value of lambda
lambda_lse <- cv.out$lambda.1se
lambda_lse

# regression coefficients
coef=coef(cv.out,s=lambda_lse)
lassocoef=as.matrix(coef(cv.out,s=lambda_lse))
write.csv(lassocoef, "lasso_coef.csv")
```

Based on the lasso regression we arrive at the lambda_min that we need to use for the logistic regression. 

Features which we need not include in the model have a coef of zero.

<p align="center">
![coef](/post/2020-03-29-lasso-logistic-regression_files/coef.jpg){width=200px height=600px}
</p>

We write the coefficients to a csv file.

The file is saved to the working directory.

If we filter on the coef columns for zero (column B), we find that 84 of the 285 features (x variables) have a coef of zero -- i.e., these have been removed by the lasso model.

The reduced set of features is used to build the logistic regression model.

We calculate the odds ratio from the coefficients of the logistic regression model:

$$ Odds\: ratio\: = exp(coef(LR\: model)) $$

The odds ratio is a very unique advantage of probability based models like logisticc regression.  The odds ratio is used to understand the influence of various x variables in classification.

```{r odds-ratio, echo=TRUE, eval=FALSE}
# Find the best lambda using cross-validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")

# Fit the final model on the dataframe
review_logreg <- glmnet(x, y, alpha = 1, family = "binomial",
                        lambda = cv.lasso$lambda.min)

# Save the regression coef to a ccsv file
logregcoef=as.matrix(coef(review_logreg))
odds_ratio=as.matrix(exp(coef(review_logreg)))
write.csv(logregcoef, "logreg_coef.csv")
write.csv(odds_ratio, "odds_ratio.csv")

```

Again, we write to a csv file for better understanding.
 
If the csv file for odds ratio is sorted based on the odds ratio (column B) in descending order, we can see the variables with the highest odds ratio

<p align="center">
![odds-ratio](/post/2020-03-29-lasso-logistic-regression_files/odds-ratio.jpg){width=200px height=600px}
</p>
The odds ratio is interpred as follows: A product which has "compliment" in its review has 5.61 times more odds to be recommended compared to a product review which does not have the word.  


We can examine the logreg_coef.csv file to understand variables with negative coefficients.

<p align="center">
![dissapoint](/post/2020-03-29-lasso-logistic-regression_files/dissapoint.jpg){width=200px height=600px}
</p>
The term "disappoint" has a negative coefficient indicating that, if the term is present in the review, the probability the product will be recommended is very low.

The 3 classification models throw similar insights in terms of what terms are used by happy customers who will recommend a product vs those use by unhappy customers.

These provide vital leads to improving product/service quality.

#### Conclusion

The article covers only the tip of the iceberg of the concepts of text mining.

The main objective of the article is to get readers introducted to the concepts of text mining.




